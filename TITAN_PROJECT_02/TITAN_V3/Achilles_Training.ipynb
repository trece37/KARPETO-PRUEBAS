{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ›¡ï¸ ACHILLES TRAINING (PHASE 4: ORO PURO) - RHLM EDITION\n",
                "**Author:** Antigravity (Google Deepmind)\n",
                "**Objective:** Train LSTM with MADL Loss & Seldon Crisis Detection.\n",
                "**Protocol:** TITAN V3 | MODE: 3-PRO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --------------------------------------------------------------------------------\n",
                "# ## NOTAS: ORQUESTACIÃ“N DE RUTAS GOOGLE DRIVE (AUTO-DETECT)\n",
                "# Este bloque asegura que no haya dudas sobre dÃ³nde estÃ¡n los archivos.\n",
                "# Busca recursivamente en la carpeta 'AchillesTraining' dentro de 'MyDrive'.\n",
                "# --------------------------------------------------------------------------------\n",
                "\n",
                "from google.colab import drive\n",
                "import os\n",
                "import glob\n",
                "import sys\n",
                "\n",
                "print(\"ðŸ¦… INICIANDO PROTOCOLO DE CONEXIÃ“N CON LA NUBE...\")\n",
                "\n",
                "# 1. Mount Drive\n",
                "drive.mount('/content/drive', force_remount=True)\n",
                "\n",
                "# 2. Intelligent Path Detection\n",
                "def find_project_root(start_path, target_folder=\"AchillesTraining\"):\n",
                "    for root, dirs, files in os.walk(start_path):\n",
                "        if target_folder in dirs:\n",
                "            return os.path.join(root, target_folder)\n",
                "    return None\n",
                "\n",
                "PROJECT_ROOT = find_project_root('/content/drive', 'AchillesTraining')\n",
                "\n",
                "if not PROJECT_ROOT:\n",
                "    print(\"âŒ ERROR CRÃTICO: No se encontrÃ³ la carpeta 'AchillesTraining' en Drive.\")\n",
                "    print(\"   -> AsegÃºrate de haber subido la carpeta factory a tu Google Drive.\")\n",
                "    sys.exit(1)\n",
                "else:\n",
                "    print(f\"âœ… PROYECTO LOCALIZADO EN: {PROJECT_ROOT}\")\n",
                "\n",
                "# 3. Define Sub-paths\n",
                "DATA_PATH = os.path.join(PROJECT_ROOT, '00_FACTORY', 'TITAN_V3', 'data')\n",
                "OUTPUT_PATH = os.path.join(PROJECT_ROOT, '00_FACTORY', 'TITAN_V3', 'output', 'v4.5')\n",
                "\n",
                "# 4. Validate Data Integrity\n",
                "if not os.path.exists(DATA_PATH):\n",
                "    print(f\"âš ï¸ DATA PATH no existe en ruta estÃ¡ndar: {DATA_PATH}\")\n",
                "    # Fallback to simple structure\n",
                "    DATA_PATH = os.path.join(PROJECT_ROOT, 'data')\n",
                "    if not os.path.exists(DATA_PATH):\n",
                "         raise FileNotFoundError(\"âŒ IMPOSIBLE LOCALIZAR CARPETA 'data'.\")\n",
                "    else:\n",
                "         print(f\"   -> Usando ruta alternativa: {DATA_PATH}\")\n",
                "\n",
                "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
                "print(f\"âœ… DATA PATH: {DATA_PATH}\")\n",
                "print(f\"âœ… OUTPUT PATH: {OUTPUT_PATH}\")\n",
                "\n",
                "# 5. Inventory check\n",
                "csv_files = glob.glob(os.path.join(DATA_PATH, \"*.csv\"))\n",
                "print(f\"ðŸ“Š Archivos CSV Detectados: {len(csv_files)}\")\n",
                "for f in csv_files:\n",
                "    print(f\"   - {os.path.basename(f)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. INSTALL & IMPORT DEPENDENCIES (RUST ENGINE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q tensorflow pandas numpy joblib scikit-learn matplotlib\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import tensorflow as tf\n",
                "import joblib\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional, Attention, Input, Multiply, Permute, Flatten\n",
                "from tensorflow.keras.models import Model\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "print(f\"TensorFlow Version: {tf.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. LOAD & FUSE DATASETS (ORO PURO)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dfs = []\n",
                "for f in csv_files:\n",
                "    print(f\"Loading {os.path.basename(f)}...\")\n",
                "    try:\n",
                "        d = pd.read_csv(f)\n",
                "        d.columns = [c.strip().lower() for c in d.columns]\n",
                "        # Basic validation\n",
                "        if 'close' in d.columns:\n",
                "            dfs.append(d)\n",
                "        else:\n",
                "            print(f\"   âš ï¸ Skipped {os.path.basename(f)} (No 'close' column)\")\n",
                "    except Exception as e:\n",
                "        print(f\"   âŒ Error reading {os.path.basename(f)}: {e}\")\n",
                "\n",
                "if not dfs:\n",
                "    raise ValueError(\"CRITICAL: No valid CSV data loaded!\")\n",
                "\n",
                "full_df = pd.concat(dfs, ignore_index=True)\n",
                "full_df.dropna(subset=['close'], inplace=True)\n",
                "print(f\"ðŸ”¥ TOTAL TRAINING ROWS: {len(full_df)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. PREPROCESSING (Dirty Hacks Pipeline)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pre-processing Pipeline for Training Data\n",
                "# 1. Select Close Price\n",
                "data = full_df['close'].values.reshape(-1, 1)\n",
                "scaler = StandardScaler()\n",
                "data_scaled = scaler.fit_transform(data)\n",
                "\n",
                "SEQ_LEN = 60\n",
                "X, y = [], []\n",
                "\n",
                "print(\"âš—ï¸ Generating Sequences via Sliding Window...\")\n",
                "for i in range(SEQ_LEN, len(data_scaled) - 5):\n",
                "    X.append(data_scaled[i-SEQ_LEN:i])\n",
                "    \n",
                "    # Labeling Logic (Future Horizon = 5 bars)\n",
                "    future_price = data[i+5][0]\n",
                "    current_price = data[i][0]\n",
                "    if current_price == 0: pct_change = 0\n",
                "    else: pct_change = (future_price - current_price) / current_price\n",
                "    \n",
                "    # 3-Class Classification\n",
                "    if pct_change > 0.005: label = [0, 1, 0] # BUY (0.5% gain)\n",
                "    elif pct_change < -0.005: label = [0, 0, 1] # SELL (-0.5% loss)\n",
                "    else: label = [1, 0, 0] # HOLD\n",
                "    \n",
                "    y.append(label)\n",
                "\n",
                "X = np.array(X)\n",
                "y = np.array(y)\n",
                "print(f\"âœ… Training Data Shape: {X.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. MODEL DEFINITION (ACHILLES LSTM V3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_achilles_v3_model(input_shape):\n",
                "    # Attention Block\n",
                "    def attention_block(inputs, time_steps):\n",
                "        a = Permute((2, 1))(inputs)\n",
                "        a = Dense(time_steps, activation='softmax')(a)\n",
                "        a_probs = Permute((2, 1))(a)\n",
                "        return Multiply()([inputs, a_probs])\n",
                "\n",
                "    inputs = Input(shape=input_shape)\n",
                "    \n",
                "    # Bi-LSTM Layer 1\n",
                "    lstm_out = Bidirectional(LSTM(128, return_sequences=True))(inputs)\n",
                "    lstm_out = BatchNormalization()(lstm_out)\n",
                "    lstm_out = Dropout(0.3)(lstm_out)\n",
                "    \n",
                "    # Bi-LSTM Layer 2\n",
                "    lstm_out = LSTM(64, return_sequences=True)(lstm_out)\n",
                "    lstm_out = BatchNormalization()(lstm_out)\n",
                "    lstm_out = Dropout(0.3)(lstm_out)\n",
                "    \n",
                "    # Attention\n",
                "    attention_mul = attention_block(lstm_out, input_shape[0])\n",
                "    attention_mul = Flatten()(attention_mul)\n",
                "    \n",
                "    # Dense Decision\n",
                "    dense = Dense(64, activation='relu')(attention_mul)\n",
                "    dense = Dropout(0.2)(dense)\n",
                "    \n",
                "    # Output\n",
                "    outputs = Dense(3, activation='softmax')(dense)\n",
                "    \n",
                "    model = Model(inputs=inputs, outputs=outputs)\n",
                "    \n",
                "    try:\n",
                "        from tensorflow.keras.optimizers import AdamW\n",
                "    except ImportError:\n",
                "        from tensorflow.keras.optimizers.experimental import AdamW\n",
                "        \n",
                "    optimizer = AdamW(learning_rate=0.001)\n",
                "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
                "    return model\n",
                "\n",
                "print(\"ðŸ¤– ACHILLES V3 ARCHITECTURE: READY.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. TRAINING EXECUTION"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = build_achilles_v3_model((SEQ_LEN, 1))\n",
                "\n",
                "callback_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
                "callback_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
                "    filepath=os.path.join(OUTPUT_PATH, 'best_achilles_v3.keras'),\n",
                "    monitor='accuracy',\n",
                "    save_best_only=True\n",
                ")\n",
                "\n",
                "print(\"ðŸš€ STARTING TRAINING (3-PRO MODE)...\")\n",
                "history = model.fit(\n",
                "    X, y, \n",
                "    epochs=20, \n",
                "    batch_size=32, \n",
                "    validation_split=0.2,\n",
                "    callbacks=[callback_stop, callback_checkpoint]\n",
                ")\n",
                "\n",
                "joblib.dump(scaler, os.path.join(OUTPUT_PATH, 'achilles_scaler.pkl'))\n",
                "print(\"ðŸ’¾ SCALER SAVED.\")\n",
                "print(\"ðŸ† TRAINING COMPLETE.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}